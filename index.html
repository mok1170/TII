<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <meta name="description" content="D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer" />
  <style>
    :root{--accent:#1f2937;--brand:#111827;--muted:#6b7280}
    html,body{height:100%}
    body{font-family:Inter,ui-sans-serif,system-ui,-apple-system,"Segoe UI",Roboto,'Helvetica Neue',Arial; background:#fff; color:var(--brand);}
    /* Force full two-end justification for all textual elements as requested */
    body, h1, h2, h3, h4, p, li, td, th, figcaption, nav, footer, a { text-align: justify; text-justify: inter-word; }

    .container{max-width:1100px}
    .shadow-soft{box-shadow:0 8px 30px rgba(17,24,39,0.06)}
    .glass{background:linear-gradient(180deg, rgba(255,255,255,0.9), rgba(250,250,250,0.9));backdrop-filter:blur(4px)}
    .figure-img{max-height:520px;object-fit:contain}
    .caption{color:var(--muted);font-size:0.95rem}
    /* OpenVLA-like simple, research-project look: restrained spacing and clean tables */
    header a.code-link{display:inline-block;padding:8px 12px;border-radius:8px;background:transparent;border:1px solid #e6e6e6;font-size:0.9rem}
    table th{background:#fafafa}
    /* make headings visually prominent while still justified */
    h1{font-weight:800; font-size:2.6rem; margin:0; line-height:1.05}
    .mute-small{color:var(--muted);font-size:0.95rem}

  </style>
</head>
<body class="antialiased text-slate-900">
  <!-- Top bar with Code link similar to OpenVLA -->
  <nav class="border-b py-3">
    <div class="container mx-auto px-5 flex items-center justify-between">
      <div class="flex items-center gap-3">
        <div class="w-10 h-10 rounded-lg bg-gray-900 flex items-center justify-center text-white font-bold">D2</div>
        <div>
          <div class="text-sm font-semibold">D2TriPO-DETR</div>
          <div class="mute-small">Dual-Decoder Triple-Parallel-Output Detection Transformer</div>
        </div>
      </div>
      <div class="">
        <a class="code-link" href="https://github.com/Embodied-Soft-Intelligence/TII" target="_blank" rel="noopener noreferrer">Code</a>
      </div>
    </div>
  </nav>

  <header class="container mx-auto px-5 py-10">
    <h1 class="leading-tight">D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer</h1>
    <p class="mt-3 mute-small">Author(s): <span class="italic">(place author names here)</span></p>

    <div class="mt-6 bg-white p-4 rounded shadow-soft">
      <p class="leading-relaxed">Vision-based grasping, though widely employed for industrial and household applications, still struggles with object stacking scenarios. Current methods face three major challenges: <strong>1)</strong> limited inter-object relationship understanding, <strong>2)</strong> poor grasping adaptation across viewpoints, and <strong>3)</strong> error propagation. We propose <strong>D2TriPO-DETR</strong>, a dual-decoder transformer that outputs three parallel results — object detection, manipulation relationship, and grasp detection — using distributed attention perception and visual attention adaptation to address these issues. On the Visual Manipulation Relationship Dataset our method outperforms prior work across metrics and demonstrates strong real-world performance.</p>
    </div>
  </header>

  <main class="container mx-auto px-5 pb-16">

    <!-- Fig 1 (no caption now) -->
    <section id="fig1" class="mt-6">
      <div class="bg-white rounded-lg p-4 shadow-soft">
        <img src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%201.jpg" alt="Fig.1" class="w-full figure-img rounded" />
      </div>
    </section>

    <!-- Removed Fig.3 as requested -->

    <!-- Spacer -->
    <div class="my-8"></div>

    <!-- Fig 7 & description -->
    <section class="grid grid-cols-1 md:grid-cols-2 gap-6 items-start">
      <div class="bg-white rounded-lg p-4 shadow-soft text-center">
        <img src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%207.jpg" alt="Fig.7" class="w-full rounded figure-img" />
      </div>
      <div class="glass p-6 rounded-lg shadow-soft">
        <h3 class="font-semibold text-lg">Experiment setup</h3>
        <p class="mt-3 leading-relaxed">描述文字（在此处填写关于 Fig. 7 的说明）。</p>
      </div>
    </section>

    <!-- Fig 8 centered with description -->
    <section class="mt-10">
      <div class="bg-white rounded-lg p-6 shadow-soft">
        <img src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%208.jpg" alt="Fig.8" class="w-full figure-img rounded" />
        <p class="mt-4 leading-relaxed">To evaluate applicability to real-world grasping, we test configurations with 2–5 objects. For each object count we run five trials in both cluttered and stacked settings, yielding 40 experiments in total.</p>
      </div>
    </section>

    <!-- Tables side-by-side -->
    <section id="results" class="mt-12 grid grid-cols-1 lg:grid-cols-2 gap-6">
      <div class="bg-white p-6 rounded-lg shadow-soft overflow-auto">
        <h4 class="font-semibold mb-4">Model results</h4>
        <div class="rounded overflow-hidden border">
          <table class="w-full text-sm">
            <thead><tr><th class="px-4 py-2 text-left">Models</th><th class="px-4 py-2">Cluttered (%)</th><th class="px-4 py-2">Stacked (%)</th></tr></thead>
            <tbody>
              <tr class="odd:bg-white even:bg-slate-50"><td class="px-4 py-2">Mutli-Task CNN [8]</td><td class="px-4 py-2 text-center">90.60</td><td class="px-4 py-2 text-center">65.65</td></tr>
              <tr class="odd:bg-white even:bg-slate-50"><td class="px-4 py-2">SMTNet [30]</td><td class="px-4 py-2 text-center">86.13</td><td class="px-4 py-2 text-center">65.00</td></tr>
              <tr class="odd:bg-white even:bg-slate-50"><td class="px-4 py-2">EGNet [5]</td><td class="px-4 py-2 text-center">93.60</td><td class="px-4 py-2 text-center">69.60</td></tr>
              <tr class="font-semibold bg-gradient-to-r from-indigo-50 to-white"><td class="px-4 py-2">D2TriPO-DETR (Ours)</td><td class="px-4 py-2 text-center">95.71</td><td class="px-4 py-2 text-center">74.29</td></tr>
            </tbody>
          </table>
        </div>
      </div>

      <div class="bg-white p-6 rounded-lg shadow-soft overflow-auto">
        <h4 class="font-semibold mb-4">Per-object success rates</h4>
        <div class="rounded overflow-hidden border">
          <table class="w-full text-sm">
            <thead><tr><th class="px-4 py-2">Objects</th><th class="px-4 py-2">2</th><th class="px-4 py-2">3</th><th class="px-4 py-2">4</th><th class="px-4 py-2">5</th></tr></thead>
            <tbody>
              <tr class="odd:bg-white even:bg-slate-50"><td class="px-4 py-2">Cluttered Scenes</td><td class="px-4 py-2 text-center">100.00%</td><td class="px-4 py-2 text-center">100.00%</td><td class="px-4 py-2 text-center">95.00%</td><td class="px-4 py-2 text-center">92.00%</td></tr>
              <tr class="odd:bg-white even:bg-slate-50"><td class="px-4 py-2">Stacked Scenes</td><td class="px-4 py-2 text-center">90.00%</td><td class="px-4 py-2 text-center">73.33%</td><td class="px-4 py-2 text-center">75.00%</td><td class="px-4 py-2 text-center">68.00%</td></tr>
            </tbody>
          </table>
        </div>
      </div>
    </section>

    <!-- Concise descriptive paragraph -->
    <section class="mt-10 bg-white p-6 rounded-lg shadow-soft">
      <h4 class="font-semibold">Experiment summary</h4>
      <p class="mt-3 leading-relaxed">In the <strong>cluttered</strong> setting objects are placed separately to test detection among nearby items. The <strong>stacked</strong> setting introduces occlusions that require detection, grasp estimation, and reasoning about manipulation order. Our pipeline outputs detections, grasp candidates, and stacking relations in parallel; detections map to fixed queries to produce object indices and update an adjacency matrix P. If P is all-zero all objects are selectable; otherwise successive powers of P identify top-layer candidates. We keep grasp candidates with IoU &gt; 0.5, rank them by confidence, and execute high-confidence candidates via inverse kinematics with closed-loop verification, updating P after each grasp–place cycle. D2TriPO-DETR shows superior relation recognition and sequence-planning, enabling robust ordered grasping in stacked scenes.</p>
    </section>

    <!-- Fig 9 + Fig 10 sections -->
    <section id="real" class="mt-10 grid grid-cols-1 md:grid-cols-2 gap-6 items-start">
      <div class="bg-white p-6 rounded-lg shadow-soft">
        <img src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%209.jpg" alt="Fig.9" class="w-full rounded figure-img" />
      </div>
      <div class="glass p-6 rounded-lg shadow-soft">
        <h4 class="font-semibold">Stability evaluation</h4>
        <p class="mt-3 leading-relaxed">We selected representative scenarios with 2–5 objects and repeated each 30 times. The system shows only a modest drop in success rate as object count increases, maintaining high and stable performance across scales.</p>
      </div>

      <div class="order-last md:order-none bg-white p-6 rounded-lg shadow-soft">
        <img src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%2010.jpg" alt="Fig.10" class="w-full rounded figure-img" />
      </div>
      <div class="glass p-6 rounded-lg shadow-soft">
        <h4 class="font-semibold">Robustness under boundary ambiguity</h4>
        <p class="mt-3 leading-relaxed">Under severe occlusion, similar colors, and insertion cases we observe grasp success rates of roughly 65%, 70%, and 60% respectively. Although boundary ambiguity reduces performance, the model still detects objects and infers stacking relations for most scenes, enabling successful grasps.</p>
      </div>
    </section>

    <!-- Video -->
    <section id="video" class="mt-12 text-center">
      <div class="bg-white p-6 rounded-lg shadow-soft inline-block">
        <p class="text-sm text-gray-600 mb-3">Demonstration video — real robot experiments</p>
        <video controls class="w-full max-w-2xl rounded" src="https://raw.githubusercontent.com/mok1170/TII/main/2.mp4">Your browser does not support the video tag.</video>
      </div>
    </section>

    <footer class="mt-12 text-center text-sm text-gray-500">© 2025 D2TriPO-DETR — Research demonstration</footer>
  </main>

</body>
</html>
