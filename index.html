<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <meta name="description" content="D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer" />
  <style>
    body { background: #f8fafc; }
    .content { max-width: 1100px; }
    .fig { max-width: 900px; }
    table { border-collapse: collapse; }
    table th, table td { border: 1px solid #e6e7eb; padding: 8px 12px; }
  </style>
</head>
<body class="antialiased text-slate-800">
  <header class="py-8">
    <div class="mx-auto content px-4">
      <h1 class="text-3xl sm:text-4xl font-extrabold text-center">D2TriPO-DETR: Dual-Decoder Triple-Parallel-Output Detection Transformer</h1>
      <p class="mt-4 text-center text-sm text-slate-600">Author(s): <span class="italic text-slate-700">(place author names here)</span></p>
    </div>
  </header>

  <main class="mx-auto content px-4 pb-12">

    <!-- Fig.1 -->
    <section class="mt-6 text-center">
      <img class="mx-auto fig rounded shadow-sm" src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%201.jpg" alt="Fig. 1" />
    </section>

    <!-- Abstract/Intro paragraph -->
    <section class="mt-6">
      <p class="text-base leading-7 text-slate-700">Vision-based grasping, though widely employed for industrial and household applications, still struggles with object stacking scenarios. Current methods face three major challenges: <strong>1)</strong> limited inter-object relationship understanding, <strong>2)</strong> poor grasping adaptation across different viewpoints, and <strong>3)</strong> error propagation. Inspired by distributed perception and visual adaptation in the human visual attention system, we propose <strong>D2TriPO-DETR</strong>, a dual-decoder transformer that produces three parallel outputs — object detection, manipulation relationship, and grasp detection — to address these challenges. A <em>distributed attention perception</em> module and a <em>visual attention adaptation</em> module are integrated into the two parallel decoders to improve inter-object relational reasoning and viewpoint adaptation respectively; the triple outputs are produced simultaneously, intrinsically reducing error propagation. On the Visual Manipulation Relationship Dataset, D2TriPO-DETR outperforms prior methods across key metrics (e.g., +5.3% detection precision, +6.7% relationship image accuracy, +1.5% grasp accuracy). Real-world tests confirm its effectiveness for grasping stacked objects.</p>
    </section>

    <!-- Fig.3 -->
    <section class="mt-8 text-center">
      <img class="mx-auto fig rounded shadow-sm" src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%203.jpg" alt="Fig. 3" />
    </section>

    <!-- Description placeholder -->
    <section class="mt-6">
      <div class="bg-white p-4 rounded shadow-sm text-slate-700">描述文字（在此处填写关于 Fig. 3 的说明）</div>
    </section>

    <!-- Spacer -->
    <div class="my-8"></div>

    <!-- Fig.7 left, description right -->
    <section class="grid grid-cols-1 md:grid-cols-2 gap-6 items-start mt-6">
      <div class="text-center">
        <img class="mx-auto max-w-full rounded shadow-sm" src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%207.jpg" alt="Fig. 7" />
      </div>
      <div class="bg-white p-4 rounded shadow-sm text-slate-700">描述文字（在此处填写关于 Fig. 7 的说明）</div>
    </section>

    <!-- Fig.8 centered with description below -->
    <section class="mt-8 text-center">
      <img class="mx-auto fig rounded shadow-sm" src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%208.jpg" alt="Fig. 8" />
      <p class="mt-3 text-slate-700 max-w-3xl mx-auto leading-7">To evaluate the applicability of our method to real-world grasping tasks, we conduct multiple experiments on a real robot platform. We test configurations with 2–5 objects (covering simple two-object cases and complex multi-object stacks). For each object count we run five trials in both cluttered and stacked settings, yielding 40 experiments in total.</p>
    </section>

    <!-- Two tables side-by-side -->
    <section class="mt-10 grid grid-cols-1 lg:grid-cols-2 gap-6">
      <div class="bg-white p-4 rounded shadow-sm overflow-auto">
        <h3 class="font-semibold mb-3">Model results</h3>
        <table class="w-full text-sm">
          <thead class="bg-slate-50"><tr><th>Models</th><th>Cluttered Scenes (%)</th><th>Stacked Scenes (%)</th></tr></thead>
          <tbody>
            <tr><td>Mutli-Task CNN [8]</td><td>90.60</td><td>65.65</td></tr>
            <tr><td>SMTNet [30]</td><td>86.13</td><td>65.00</td></tr>
            <tr><td>EGNet [5]</td><td>93.60</td><td>69.60</td></tr>
            <tr class="font-semibold"><td>D2TriPO-DETR (Ours)</td><td>95.71</td><td>74.29</td></tr>
          </tbody>
        </table>
      </div>

      <div class="bg-white p-4 rounded shadow-sm overflow-auto">
        <h3 class="font-semibold mb-3">Per-object success rates</h3>
        <table class="w-full text-sm">
          <thead class="bg-slate-50"><tr><th>Objects</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead>
          <tbody>
            <tr><td>Cluttered Scenes</td><td>100.00%</td><td>100.00%</td><td>95.00%</td><td>92.00%</td></tr>
            <tr><td>Stacked Scenes</td><td>90.00%</td><td>73.33%</td><td>75.00%</td><td>68.00%</td></tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- Simplified descriptive paragraph (user asked to be concise) -->
    <section class="mt-8">
      <p class="text-slate-700 leading-7">In the <strong>cluttered</strong> setting objects are placed separately at random positions to evaluate detection among closely spaced items. The <strong>stacked</strong> setting includes occlusions from stacked objects, requiring detection, grasp estimation, and reasoning about manipulation ordering to avoid failures. Our pipeline first outputs detections, grasp candidates, and stacking relations in parallel; detections map to fixed queries to generate object indices and update an adjacency matrix P. If P is all-zero all objects are selectable; otherwise we compute successive powers of P to determine top-layer candidates. Grasp candidates with IoU &gt; 0.5 to their box are retained and ranked by confidence; execution prioritizes high-confidence candidates in the candidate set and uses inverse kinematics with closed-loop verification. After each grasp-place cycle P is updated until task completion. Compared to mainstream methods, D2TriPO-DETR shows superior relation recognition and sequence-planning, enabling stable, correctly ordered grasping in real stacked environments.</p>
    </section>

    <!-- Fig.9 left, description right -->
    <section class="mt-8 grid grid-cols-1 md:grid-cols-2 gap-6 items-start">
      <div class="text-center md:text-left">
        <img class="mx-auto md:mx-0 max-w-full rounded shadow-sm" src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%209.jpg" alt="Fig. 9" />
      </div>
      <div class="bg-white p-4 rounded shadow-sm text-slate-700">To quantify stability we select one representative scenario for each object count (2, 3, 4, 5) and repeat each 30 times under identical conditions. The system shows only a slight drop in success rate as object number increases, maintaining high and stable grasping performance across scales.</div>
    </section>

    <!-- Fig.10 right, description left -->
    <section class="mt-8 grid grid-cols-1 md:grid-cols-2 gap-6 items-start">
      <div class="order-2 md:order-1 bg-white p-4 rounded shadow-sm text-slate-700">When objects are severely occluded they lose local distinguishing features; similar colors and close contours further complicate discrimination; insertion causes depth ambiguity. We evaluate robustness with three controlled tests (severe occlusion, similar color, insertion) each containing 20 scenes. Grasping success rates are approximately 65%, 70%, and 60% respectively — showing that despite boundary ambiguity the model still performs detection and relation inference for most samples and can complete grasps.</div>
      <div class="order-1 md:order-2 text-center md:text-right">
        <img class="mx-auto md:ml-auto max-w-full rounded shadow-sm" src="https://raw.githubusercontent.com/mok1170/TII/main/Fig.%2010.jpg" alt="Fig. 10" />
      </div>
    </section>

    <!-- Video -->
    <section class="mt-10 text-center">
      <p class="text-sm text-slate-600 mb-2">Demonstration video (real robot experiments)</p>
      <video controls class="mx-auto max-w-full rounded shadow-sm" src="https://raw.githubusercontent.com/mok1170/TII/main/2.mp4">Your browser does not support the video tag.</video>
    </section>

    <footer class="mt-12 text-center text-xs text-slate-500">© 2025 D2TriPO-DETR — Content for research demonstration</footer>

  </main>

</body>
</html>
